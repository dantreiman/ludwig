#!/usr/bin/env python

# Synthetic Data Training Example
#
# This example constructs a synthetic dataset of of input features using numerical, categorical, and binary features.
# Each output feature is generated by applying a function to a pair of input features.
#

import logging

from ludwig.api import LudwigModel
import numpy as np
import pandas as pd
import tqdm


DATASET_SIZE = 100000  # Size of synthetic generated dataset.
MISSING_VALUE_FRAC = .1  # Fraction of values to nullify in columns with missing values.
RUNS = 10  # Run every experiment this many times.


def train_test_val_split(df, test_fraction=0.1, val_fraction=0.1):
    test_size = int(len(df) * 0.1)
    val_size = int(len(df) * 0.1)
    df_shuffled = df.sample(frac=1, random_state=2021)
    return (
        df_shuffled[test_size + val_size:],
        df_shuffled[:test_size],
        df_shuffled[test_size:test_size+val_size],
    )


def make_noisy_arithmetic_dataset(n_rows, operators=['add', 'avg'], input_range=10.0, noise=1.0):
    """Creates a synthetic dataset of features.
       Random Features:
         a: random real from 0 to input_range
         b: random real from 0 to input_range
         op: random operation from ['add', 'sub']
         o: random integer from [0, 1]
         n: random gaussian noise
       Derived Features:
         am: a with 10% missing values (Nan)
         bm: b with 10% missing values (Nan)
         opm: op with 10% missing values (empty strings)
         om: o with 10% missing values (Nan)
         c: numerical = a <op> b + n
         p: binary = c > 0
         e: categorical = (['even', 'odd']) if c rounded to the nearest integer is even or odd.
    """
    rng = np.random.default_rng(2021)
    a = rng.random(DATASET_SIZE, dtype=np.float32) * input_range
    b = rng.random(DATASET_SIZE, dtype=np.float32) * input_range
    o = rng.choice([0, 1], DATASET_SIZE)
    op = np.array([operators[x] for x in o], dtype=str)
    N = (noise * rng.normal(size=DATASET_SIZE))
    am = a.copy()
    am[rng.choice(DATASET_SIZE, int(DATASET_SIZE * MISSING_VALUE_FRAC), replace=False)] = np.nan
    bm = b.copy()
    bm[rng.choice(DATASET_SIZE, int(DATASET_SIZE * MISSING_VALUE_FRAC), replace=False)] = np.nan
    opm = op.copy()
    opm[rng.choice(DATASET_SIZE, int(DATASET_SIZE * MISSING_VALUE_FRAC), replace=False)] = ''
    om = op.copy()
    om[rng.choice(DATASET_SIZE, int(DATASET_SIZE * MISSING_VALUE_FRAC), replace=False)] = np.nan
    c = np.select([op == 'add', op == 'avg'], [a+b, (a+b)/2]) + N  # a <op> b + gaussian noise
    p = c > 1
    c_nearest_int = np.round(c).astype(np.int32)
    e = ['even' if i % 2 == 0 else 'odd' for i in c_nearest_int]
    return pd.DataFrame({
        'a': a,
        'b': b,
        'op': op,
        'o': o,
        'n': N,
        'am': am,
        'bm': bm,
        'opm': opm,
        'om': om,
        'c': c,
        'p': p,
        'e': e
    })


print('Generating synthetic training data')
dataset_df = make_noisy_arithmetic_dataset(DATASET_SIZE)
training_set, test_set, val_set = train_test_val_split(dataset_df)
print('training_set.head():')
print(training_set.head())
print('')


OPTIMIZERS = [
    'sgd',
    'adam'
]

IN_AXES = {
    'numeric': [{'name':'a', 'type':'numerical'}, {'name':'b', 'type':'numerical'}],
    'numeric_missing': [{'name':'am', 'type':'numerical'}, {'name':'bm', 'type':'numerical'}],
    'category_str': [{'name':'a', 'type':'numerical'}, {'name':'b', 'type':'numerical'}, {'name':'op', 'type':'category'}],
    'category_str_missing': [{'name':'a', 'type':'numerical'}, {'name':'b', 'type':'numerical'}, {'name':'opm', 'type':'category'}],
    'category_int': [{'name':'a', 'type':'numerical'}, {'name':'b', 'type':'numerical'}, {'name':'o', 'type':'category'}],
    'category_int_missing': [{'name':'a', 'type':'numerical'}, {'name':'b', 'type':'numerical'}, {'name':'om', 'type':'category'}],
    'binary': [{'name':'a', 'type':'numerical'}, {'name':'b', 'type':'numerical'}, {'name':'o', 'type':'binary'}],
    'binary_missing': [{'name': 'a', 'type': 'numerical'}, {'name': 'b', 'type': 'numerical'}, {'name': 'o', 'type': 'binary'}],
}

OUT_AXES = {
    'numeric': [{'name':'c', 'type':'numerical'}],
    'category': [{'name':'e', 'type':'category'}],
    'binary': [{'name':'p', 'type':'binary'}]
}


def make_model_config(input_features, output_features, optimizer='sgd'):
    return {
        'input_features': input_features,
        'output_features': output_features,
        'training': {
            'epochs': 10,
            'learning_rate': 0.05,
            'optimizer': {
                'type': optimizer,
            },
            'decay': False,
            'early_stop': 0,
            'regularization_lambda': 0
        }
    }

results = {
    'run': [],
    'model_name': [],
    'optimizer': [],
    'in_axis': [],
    'out_axis': [],
    'first_train_loss': [],
    'first_test_loss': [],
    'first_validation_loss': [],
    'final_train_loss': [],
    'final_test_loss': [],
    'final_validation_loss': []
}

for ri in tqdm.tqdm(range(RUNS)):
    for opt_i, opt in enumerate(OPTIMIZERS):
        for ii, (in_axis, in_config) in enumerate(IN_AXES.items()):
            for oi, (out_axis, out_config) in enumerate(OUT_AXES.items()):
                model_name = f"{opt}_{in_axis}_in_{out_axis}_out"
                config = make_model_config(in_config, out_config, opt)
                model = LudwigModel(config=config, logging_level=logging.ERROR)

                train_stats, _, _ = model.train(
                    training_set=training_set,
                    validation_set=val_set,
                    test_set=test_set,
                    experiment_name="synthetic_data_experiment",
                    model_name=model_name,
                    output_directory='./results',
                    random_seed=2021
                )
                train_loss = train_stats['training']['combined']['loss']
                test_loss = train_stats['test']['combined']['loss']
                validation_loss = train_stats['validation']['combined']['loss']
                results['run'].append(ri)
                results['model_name'].append(model_name)
                results['optimizer'].append(opt)
                results['in_axis'].append(in_axis)
                results['out_axis'].append(out_axis)
                results['first_train_loss'].append(train_loss[0])
                results['first_test_loss'].append(test_loss[0])
                results['first_validation_loss'].append(validation_loss[0])
                results['final_train_loss'].append(train_loss[-1])
                results['final_test_loss'].append(test_loss[-1])
                results['final_validation_loss'].append(validation_loss[-1])


results_df = pd.DataFrame(results)
results_df.to_csv('feature_combinations_all_runs.csv', index=False)

mean_results_df = results_df[[
    'model_name', 'first_train_loss', 'first_test_loss', 'first_validation_loss',
    'final_train_loss', 'final_test_loss', 'final_validation_loss']].groupby('model_name').mean()
mean_results_df.to_csv('feature_combinations_averages.csv', index=True)
